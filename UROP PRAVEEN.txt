FEDERATED LEARNING:
   The core idea behind federated learning is decentrlized learing,where the user data is never sent to the central server.Data is often created on edge devices such as smartphones or IOT sensors.Federated learning is asolution that allows in device machine learning without transferring the user's private data to a central cloud.

INTERNET OF THINGS:
   For IOT, along with cloud,fog,dew,edege computing is impotant,that provides aggregation and data processing from sensor networks and individual devices related to the physical world.It is a network of connected smart devices providing rich data, but it can also be a security nightmare.

INTERNET OF THINGS - FEDERATED LEARNING:
   Federated learning can help achieve peronalization.As well as enhance the performance of device in IOT applications.Federated learning traning framework in IOT,big data and multimedia communicatins.Federated learning is a feasible solution to solve the problems of data islands, break data barriers, and protect data security and privacy, especially in the context of the IoT, and big data. Distributed IoT and big data users need to collaboratively train a classification or regression model to implement perfect data prediction results without compromising privacy. Unlike privacy-preserving outsourced training, rather than submitting data to the centralized cloud server, users train data locally in FL. The federated center is only responsible for aggregating the gradient information (or model parameters) uploaded by users and distributing the global training model.

APPLICATION:

  Communication-efficient Federated Learning For Wireless Edge Intelligence In IoT

    The rapidly expanding number of Internet of Things devices is generating huge quantities of data, but public concern over data privacy means users are apprehensive to send data to a central server for machine learning purposes. The easily changed behaviors of edge infrastructure that software-defined networking provides makes it possible to collate IoT data at edge servers and gateways, where federated learning can be performed: building a central model without uploading data to the server. FedAvg is an federated learning algorithm which has been the subject of much study, however, it suffers from a large number of rounds to convergence with non-independent identically distributed (non-IID) client data sets and high communication costs per round. We propose adapting FedAvg to use a distributed form of Adam optimization, greatly reducing the number of rounds to convergence, along with the novel compression techniques, to produce communication-efficient FedAvg (CE-FedAvg). We perform extensive experiments with the MNIST/CIFAR-10 data sets, IID/non-IID client data, varying numbers of clients, client participation rates, and compression rates. These show that CE-FedAvg can converge to a target accuracy in up to  6xless rounds than similarly compressed FedAvg, while uploading up to 3xless data, and is more robust to aggressive compression. Experiments on an edge-computing-like testbed using Raspberry Pi clients also show that CE-FedAvg is able to reach a target accuracy in up to 1.7xless real time than FedAvg.
